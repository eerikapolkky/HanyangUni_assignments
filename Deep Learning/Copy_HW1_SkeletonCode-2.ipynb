{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKelMv2k5gNx",
        "outputId": "80f1f153-c04c-4a15-9946-cbc9be5bd48a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file \"hw1_data.tsv\"\n",
        "file = open(\"/content/drive/MyDrive/Colab Notebooks/hw1_data.tsv\",'r')\n",
        "ori_data = file.read().strip().split(\"\\n\")\n",
        "data = []\n",
        "for item in ori_data:\n",
        "  item = item.split(\"\\t\")\n",
        "  if len(item[0]) >0:\n",
        "    data.append((item[0],item[1]))\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObQrvnbB8hPZ",
        "outputId": "ec887519-5a70-44d9-db4f-79ec60888bdf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSiVjnSKBlcK",
        "outputId": "18fc6fec-8912-4303-ea7f-d8fecd5cbcdd"
      },
      "source": [
        "### Training data pre-processing\n",
        "texts, labels = [], []\n",
        "label2idx = {\"0\":0, \"1\":1}\n",
        "for i, item in enumerate(data):\n",
        "  text = item[0]\n",
        "\n",
        "  ## Preprocessing (if you want to add, please add more)\n",
        "  ################################################################################\n",
        "  text = text.replace(\"  \",\" \") ## Replace double space\n",
        "  text = text.replace(\",\", \"\") ## Replace comma to \"\"\n",
        "  text = text.lower()  ## Lower cases\n",
        "  ################################################################################\n",
        "  label = label2idx[item[1]]\n",
        "\n",
        "  texts.append(text)\n",
        "  labels.append(label)\n",
        "\n",
        "print(\"*\"*50)\n",
        "print(\"Total number of datasets\")\n",
        "print(len(texts))\n",
        "print(len(labels))\n",
        "print(\"*\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Total number of datasets\n",
            "300\n",
            "300\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Split into train/dev/test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "### Write a code for collecting samples for each class\n",
        "################################################################################\n",
        "pos,neg = [], []\n",
        "train_texts, dev_texts, test_texts, train_labels, dev_labels, test_labels = [], [], [], [], [], []\n",
        "for a,b in zip(texts,labels):\n",
        "  if b == 1:\n",
        "    pos.append((a,b))\n",
        "  else:\n",
        "    neg.append((a,b))\n",
        "\n",
        "for a,b in pos[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "for a,b in neg[:50]:\n",
        "  test_texts.append(a)\n",
        "  test_labels.append(b)\n",
        "\n",
        "for a,b in pos[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "for a,b in neg[50:100]:\n",
        "  dev_texts.append(a)\n",
        "  dev_labels.append(b)\n",
        "\n",
        "for a,b in pos[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "for a,b in neg[100:]:\n",
        "  train_texts.append(a)\n",
        "  train_labels.append(b)\n",
        "################################################################################\n",
        "\n",
        "\n",
        "tmp = list(zip(train_texts,train_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "train_texts, train_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(dev_texts,dev_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "dev_texts, dev_labels = zip(*tmp)\n",
        "\n",
        "tmp = list(zip(test_texts,test_labels))\n",
        "import random\n",
        "random.shuffle(tmp)\n",
        "test_texts, test_labels = zip(*tmp)\n",
        "\n",
        "print(\"Train Dataset Examples\")\n",
        "print(train_texts[:3])\n",
        "print(train_labels[:3])\n",
        "print(\"*\"*50)\n",
        "print(train_labels)\n",
        "print(dev_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZyzodaGovkH",
        "outputId": "69614d43-3787-444a-c108-03f86673c79f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Examples\n",
            "('handsome but unfulfilling suspense drama ', \"the rock 's fighting skills are more in line with steven seagal \", \"as a dentist 's waiting room \")\n",
            "(0, 1, 0)\n",
            "**************************************************\n",
            "(0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1)\n",
            "(0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4wmL0jRBw2_",
        "outputId": "17720972-8ba7-479c-ddce-7659b89d8d90"
      },
      "source": [
        "## Construct a vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "all_words = []\n",
        "for item in train_texts:\n",
        "  all_words += item.split()\n",
        "for item in dev_texts:\n",
        "  all_words += item.split()\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(all_words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {'<pad>':0, \"<unk>\":1}\n",
        "vocab_to_int.update({word: ii for ii, word in enumerate(vocab,2)})\n",
        "print(vocab_to_int)\n",
        "print(len(vocab_to_int))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, '<unk>': 1, 'the': 2, 'and': 3, 'a': 4, '.': 5, 'to': 6, 'of': 7, 'it': 8, 'in': 9, 'that': 10, 'is': 11, \"'s\": 12, 'as': 13, 'for': 14, 'with': 15, 'film': 16, 'you': 17, 'one': 18, 'movie': 19, 'by': 20, 'this': 21, 'its': 22, '--': 23, 'have': 24, '...': 25, 'good': 26, 'so': 27, 'story': 28, 'more': 29, 'no': 30, 'not': 31, 'time': 32, 'if': 33, 'your': 34, \"n't\": 35, 'most': 36, 'but': 37, 'are': 38, 'way': 39, '(': 40, ')': 41, 'an': 42, 'very': 43, 'little': 44, 'about': 45, 'on': 46, 'i': 47, 'takes': 48, 'own': 49, 'be': 50, 'too': 51, 'worst': 52, '``': 53, 'picture': 54, 'or': 55, 'from': 56, 'some': 57, 'at': 58, 'just': 59, 'can': 60, 'could': 61, 'when': 62, 'comedy': 63, 'his': 64, 'their': 65, 'every': 66, 'like': 67, 'does': 68, 'into': 69, 'all': 70, 'recent': 71, \"''\": 72, 'many': 73, 'hate': 74, 'life': 75, 'surprisingly': 76, 'watching': 77, 'what': 78, 'better': 79, 'moments': 80, 'old': 81, 'men': 82, 'work': 83, 'completely': 84, 'suspense': 85, 'rock': 86, 'line': 87, 'uses': 88, 'sensational': 89, 'crime': 90, 'was': 91, ':': 92, 'wildly': 93, 'step': 94, 'seems': 95, 'unfortunately': 96, 'which': 97, 'point': 98, 'derivative': 99, 'epic': 100, 'new': 101, 'plot': 102, 'he': 103, 'awful': 104, 'superb': 105, 'tribute': 106, 'only': 107, 'fear': 108, 'look': 109, 'whole': 110, 'mess': 111, 'worth': 112, 'feels': 113, 'even': 114, 'sensitive': 115, 'much': 116, 'romance': 117, 'will': 118, 'laughs': 119, 'through': 120, 'small': 121, 'considerable': 122, 'aplomb': 123, 'talented': 124, 'once': 125, 'ride': 126, 'seen': 127, 'noir': 128, 'effects': 129, 'despite': 130, 'amounts': 131, 'well': 132, 'depth': 133, 'than': 134, 'who': 135, 'has': 136, 'charm': 137, 'serves': 138, 'dumb': 139, 'fun': 140, 'may': 141, 'make': 142, 'poignant': 143, 'part': 144, 'dialogue': 145, 'kind': 146, 'horror': 147, 'digital': 148, 'heart': 149, 'itself': 150, 'two': 151, 'being': 152, 'again': 153, 'late': 154, 'want': 155, 'pay': 156, 'justify': 157, 'theatrical': 158, 'simulation': 159, 'death': 160, 'camp': 161, 'auschwitz': 162, 'ii-birkenau': 163, 'best': 164, 'young': 165, 'out': 166, 'sketchy': 167, 'cinematic': 168, 'history': 169, 'cuteness': 170, 'flaws': 171, 'would': 172, 'conviction': 173, 'jason': 174, 'sometimes': 175, 'makes': 176, 'minutes': 177, 'almost': 178, 'how': 179, 'family': 180, 'beautiful': 181, 'silly': 182, 'enough': 183, 'outrageous': 184, 'think': 185, 'attractive': 186, 'watch': 187, 'after': 188, 'scene': 189, 'special': 190, 'put': 191, 'together': 192, 'year': 193, 'any': 194, 'handsome': 195, 'unfulfilling': 196, 'drama': 197, 'fighting': 198, 'skills': 199, 'steven': 200, 'seagal': 201, 'dentist': 202, 'waiting': 203, 'room': 204, 'evanescent': 205, 'seamless': 206, 'sumptuous': 207, 'stream': 208, 'acted': 209, 'diane': 210, 'lane': 211, 'richard': 212, 'gere': 213, 'real-life': 214, '19th-century': 215, 'metaphor': 216, 'produced': 217, 'jerry': 218, 'bruckheimer': 219, 'directed': 220, 'joel': 221, 'schumacher': 222, 'reflects': 223, 'shallow': 224, 'styles': 225, 'overproduced': 226, 'inadequately': 227, 'motivated': 228, 'demographically': 229, 'targeted': 230, 'please': 231, 'résumé': 232, 'loaded': 233, 'credits': 234, 'girl': 235, 'bar': 236, '#': 237, '3': 238, 'positive': 239, 'change': 240, 'tone': 241, 'here': 242, 'recharged': 243, 'him': 244, 'esther': 245, 'kahn': 246, 'unusual': 247, 'also': 248, 'irritating': 249, 'tedious': 250, 'norwegian': 251, 'offering': 252, 'somehow': 253, 'snagged': 254, 'oscar': 255, 'nomination': 256, 'adventurous': 257, 'indian': 258, 'filmmakers': 259, 'toward': 260, 'crossover': 261, 'nonethnic': 262, 'markets': 263, 'comic': 264, 'gem': 265, 'delightful': 266, 'showing': 267, 'honest': 268, 'emotions': 269, 'either': 270, 'valuable': 271, 'messages': 272, 'value': 273, 'respect': 274, 'term': 275, 'cinema': 276, 'conceptions': 277, 'because': 278, 'acts': 279, 'goofy': 280, 'slick': 281, 'manufactured': 282, 'claim': 283, 'street': 284, 'credibility': 285, 'thoroughly': 286, 'add': 287, 'beyond': 288, 'dark': 289, 'visions': 290, 'already': 291, 'relayed': 292, 'predecessors': 293, 'smooth': 294, 'professional': 295, 'warm': 296, 'water': 297, 'under': 298, 'red': 299, 'bridge': 300, 'celebration': 301, 'feminine': 302, 'energy': 303, 'power': 304, 'women': 305, 'heal': 306, 'thing': 307, 'dot': 308, 'com': 309, 'smeary': 310, 'blurry': 311, 'distraction': 312, 'bogus': 313, 'extremely': 314, 'unpleasant': 315, 'traditionally': 316, 'structured': 317, 'fascinating': 318, 'byways': 319, 'price': 320, 'admission': 321, 'gory': 322, 'mayhem': 323, 'idea': 324, 'predictable': 325, 'tides': 326, 'usual': 327, 'merit': 328, '103-minute': 329, 'length': 330, 'affair': 331, 'true': 332, 'incredibly': 333, 'hokey': 334, 'everything': 335, 'girls': 336, 'ca': 337, 'swim': 338, 'passages': 339, 'observation': 340, 'secondhand': 341, 'familiar': 342, 'frida': 343, 'different': 344, 'hollywood': 345, 'home': 346, 'leave': 347, 'wanting': 348, 'mention': 349, 'leaving': 350, 'smile': 351, 'face': 352, 'halfway': 353, 'beginning': 354, 'enables': 355, 'shafer': 356, 'navigate': 357, 'spaces': 358, 'both': 359, 'large': 360, 'essentially': 361, 'collection': 362, 'bits': 363, 'shot': 364, 'artful': 365, 'watery': 366, 'tones': 367, 'blue': 368, 'green': 369, 'brown': 370, 'entirely': 371, 'persuasive': 372, 'give': 373, 'exposure': 374, 'performers': 375, 'laughable': 376, 'compulsively': 377, 'watchable': 378, 'unassuming': 379, 'subordinate': 380, 'enjoy': 381, 'breezy': 382, 'distracted': 383, 'rhythms': 384, 'things': 385, 'we': 386, \"'ve\": 387, 'before': 388, 'unorthodox': 389, 'organized': 390, 'includes': 391, 'strangest': 392, 'guns': 393, 'cheatfully': 394, 'filmed': 395, 'martial': 396, 'arts': 397, 'disintegrating': 398, 'bloodsucker': 399, 'computer': 400, 'jagged': 401, 'camera': 402, 'moves': 403, 'hold': 404, 'grips': 405, 'hard': 406, 'disappointingly': 407, 'thin': 408, 'slice': 409, 'lower-class': 410, 'london': 411, ';': 412, 'title': 413, 'bode': 414, 'rest': 415, 'degraded': 416, 'handheld': 417, 'blair': 418, 'witch': 419, 'video-cam': 420, 'footage': 421, 'working': 422, 'script': 423, 'co-written': 424, 'gianni': 425, 'romoli': 426, 'ozpetek': 427, 'avoids': 428, 'pitfalls': 429, \"'d\": 430, 'expect': 431, 'such': 432, 'potentially': 433, 'sudsy': 434, 'set-up': 435, 'added': 436, 'resonance': 437, 'spare': 438, 'wildlife': 439, 'dim': 440, 'amazingly': 441, 'lame': 442, 'technically': 443, 'hugh': 444, 'grant': 445, 'second': 446, 'fiddle': 447, 'auto-critique': 448, 'clumsiness': 449, 'damning': 450, 'censure': 451, 'setup': 452, 'easy': 453, 'borders': 454, 'facile': 455, 'vicious': 456, 'absurd': 457, 'bewilderingly': 458, 'brilliant': 459, 'entertaining': 460, 'understand': 461, 'difference': 462, 'between': 463, 'plain': 464, 'kinetic': 465, 'teeming': 466, 'cranky': 467, 'adults': 468, 'rediscover': 469, 'quivering': 470, 'kid': 471, 'inside': 472, 'stylish': 473, 'exercise': 474, 'manages': 475, 'infuse': 476, 'rocky': 477, 'path': 478, 'sibling': 479, 'reconciliation': 480, 'flashes': 481, 'warmth': 482, 'gentle': 483, 'humor': 484, 'retains': 485, 'ambiguities': 486, 'speak': 487, 'while': 488, 'forces': 489, 'ponder': 490, 'anew': 491, 'significantly': 492, 'leavened': 493, 'thanks': 494, 'lau': 495, 'ultra-cheesy': 496, 'admittedly': 497, 'middling': 498, 'ludicrous': 499, 'breathless': 500, 'anticipation': 501, 'carnage': 502, 'elevated': 503, 'confessions': 504, 'straightforward': 505, 'bio': 506, 'rich': 507, 'full': 508, 'scant': 509, 'ugly': 510, 'shabby': 511, 'photography': 512, 'pan-american': 513, 'genuine': 514, 'insight': 515, 'urban': 516, 'folly': 517, 'superficiality': 518, 'take': 519, 'care': 520, 'nicely': 521, 'performed': 522, 'quintet': 523, 'actresses': 524, 'used': 525, 'my': 526, 'hours': 527, 'john': 528, 'malkovich': 529, 'her': 530, 'charmless': 531, 'arrive': 532, 'early': 533, 'stay': 534, 'yes': 535, 'snail-like': 536, 'pacing': 537, 'guys': 538, 'desperately': 539, 'quentin': 540, 'tarantino': 541, 'they': 542, 'grow': 543, 'up': 544, 'chances': 545, 'bold': 546, 'studio': 547, 'standards': 548, 'pays': 549, 'earnest': 550, 'homage': 551, 'turntablists': 552, 'beat': 553, 'jugglers': 554, 'schoolers': 555, 'current': 556, 'innovators': 557, 'told': 558, 'scattered': 559, 'fashion': 560, 'promise': 561, 'filmmaking': 562, 'wide-awake': 563, 'elvira': 564, 'fans': 565, 'hardly': 566, 'ask': 567, 'easily': 568, 'wait': 569, 'per': 570, 'view': 571, 'dollar': 572, 'delivers': 573, 'promises': 574, 'wild': 575, 'ensues': 576, 'brash': 577, 'set': 578, 'conquer': 579, 'online': 580, 'world': 581, 'laptops': 582, 'cell': 583, 'phones': 584, 'business': 585, 'plans': 586, 'yet': 587, 'grating': 588, 'showcase': 589, 'bon': 590, 'bons': 591, 'gender-bending': 592, 'generally': 593, 'quite': 594, 'funny': 595, 'provide': 596, 'keenest': 597, 'pleasures': 598, 'lika': 599, 'da': 600, 'harsh': 601, 'piece': 602, 'storytelling': 603, 'effective': 604, 'stick': 605, 'addition': 606, 'sporting': 607, 'titles': 608, 'proves': 609, 'lovely': 610, 'trifle': 611, 'love': 612, 'disney': 613, 'ransacks': 614, 'archives': 615, 'quick-buck': 616, 'sequel': 617, 'extraordinary': 618, 'faith': 619, 'hopeless': 620, 'inane': 621, 'challenges': 622, 'poses': 623, 'forgive': 624, 'workable': 625, 'primer': 626, 'region': 627, 'terrific': 628, '10th-grade': 629, 'learning': 630, 'tool': 631, 'eats': 632, 'meddles': 633, 'argues': 634, 'kibbitzes': 635, 'fights': 636, 'brings': 637, 'proper': 638, 'role': 639, 'bourne': 640, 'mid-to-low': 641, 'budget': 642, 'betrayed': 643, 'shoddy': 644, 'makeup': 645, 'collapse': 646, 'movies': 647, 'suck': 648, 'caruso': 649, 'descends': 650, 'sub-tarantino': 651, 'sure': 652, 'salton': 653, 'sea': 654, 'works': 655, 'should': 656, 'keeping': 657, 'tight': 658, 'nasty': 659, 'those': 660, 'so-so': 661, 'films': 662, 'been': 663, 'do': 664, 'concert': 665, 'runs': 666, 'mere': 667, '84': 668, 'glance': 669, 'black': 670, 'ii': 671, 'achieves': 672, 'ultimate': 673, 'insignificance': 674, 'sci-fi': 675, 'spectacle': 676, 'whiffle-ball': 677, 'lacking': 678, 'surprise': 679, 'consistent': 680, 'emotional': 681, 'lively': 682, 'engaging': 683, 'examination': 684, 'similar': 685, 'obsessions': 686, 'dominate': 687, 'irritates': 688, 'adaptation': 689, 'hammily': 690, 'direction': 691, 'fluid': 692, 'no-nonsense': 693, 'authority': 694, 'performances': 695, 'harris': 696, 'phifer': 697, 'cam': 698, '`': 699, 'ron': 700, 'seal': 701, 'deal': 702, 'uneven': 703, 'infectiously': 704, 'tear': 705, 'eyes': 706, 'away': 707, 'images': 708, 'long': 709, 'read': 710, 'subtitles': 711, 'ingenious': 712, 'well-written': 713, 'well-acted': 714, 'dog': 715, 'interesting': 716, 'overbearing': 717, 'over-the-top': 718, 'vainly': 719, 'pleasant': 720, 'oozing': 721, 'color': 722, 'rather': 723, 'there': 724, 'something': 725, 'artist': 726, '90-plus': 727, 'years': 728, 'taking': 729, 'effort': 730, 'share': 731, 'impressions': 732, 'loss': 733, 'art': 734, 'us': 735, 'audacious': 736, 'supremely': 737, 'unfunny': 738, 'unentertaining': 739, 'middle-age': 740, 'compelling': 741, 'plodding': 742, 'unpretentious': 743, 'charming': 744, 'quirky': 745, 'original': 746, 'well-thought': 747, 'stunts': 748, 'character': 749, 'dramas': 750, 'never': 751, 'reach': 752, 'satisfying': 753, 'conclusions': 754, 'nicks': 755, 'steinberg': 756, 'match': 757, 'creations': 758, 'pure': 759, 'venality': 760, 'giving': 761, 'college': 762, 'try': 763, 'classic': 764, 'casts': 765, 'actors': 766, 'magnificent': 767, 'landscape': 768, 'create': 769, 'feature': 770, 'wickedly': 771, 'clumsy': 772, 'heavy-handed': 773, 'phoney-feeling': 774, 'sentiment': 775, 'laugh': 776, 'maybe': 777, 'twice': 778, 'forgotten': 779, 'get': 780, 'back': 781, 'car': 782, 'parking': 783, 'lot': 784, 'liked': 785, 'had': 786, 'gone': 787, 'further': 788, 'tasteful': 789, 'roll': 790, 'final': 791, 'welcome': 792, 'relief': 793, 'passable': 794, 'date': 795, 'bring': 796, 'tissues': 797, 'absolutely': 798, 'ridiculous': 799, 'imax': 800, 'short': 801, 'reveals': 802, 'important': 803, 'our': 804, 'talents': 805, 'service': 806, 'others': 807, 'big': 808, 'excuse': 809, 'play': 810, 'lewd': 811, 'another': 812, 'believe': 813, 'actually': 814, 'backseat': 815, 'pokes': 816, 'provokes': 817, 'expressionistic': 818, 'license': 819, 'hawaiian': 820, 'shirt': 821, 'involved': 822, 'save': 823, 'dash': 824, 'shows': 825, 'slightest': 826, 'aptitude': 827, 'acting': 828, 'starts': 829, 'off': 830, 'bad': 831, 'feel': 832, 'running': 833, 'screaming': 834, 'realistic': 835, 'portrayal': 836, 'trouble': 837, 'day': 838, 'preliminary': 839, 'notes': 840, 'science-fiction': 841, 'fragmentary': 842, 'narrative': 843, 'style': 844, 'piecing': 845, 'frustrating': 846, 'difficult': 847, 'respectable': 848, 'overlong': 849, 'bombastic': 850, 'remarkable': 851, 'procession': 852, 'sweeping': 853, 'pictures': 854, 'reinvigorated': 855, 'genre': 856, 'lost': 857, 'translation': 858, 'digital-effects-heavy': 859, 'supposed': 860, 'family-friendly': 861, 'conclusive': 862, 'answers': 863, 'unpredictable': 864, 'able': 865, 'hit': 866, '15-year': 867, \"'re\": 868, 'over': 869, '100': 870, 'build': 871, 'robots': 872, 'haul': 873, \"'em\": 874, 'theater': 875, 'show': 876, 'mystery': 877, 'science': 878, 'theatre': 879, '3000': 880, 'certainly': 881, 'going': 882, 'go': 883, 'down': 884, 'killer': 885, 'website': 886, 'other': 887, 'underscore': 888, 'importance': 889, 'tradition': 890, 'familial': 891, 'community': 892, 'willing': 893, 'champion': 894, 'fallibility': 895, 'human': 896, 'grievous': 897, 'cobbled': 898, 'largely': 899, 'flat': 900, 'uncreative': 901, 'trying': 902, 'grab': 903, 'lump': 904, 'play-doh': 905, 'harder': 906, 'liman': 907, 'tries': 908, 'squeeze': 909, 'woman': 910, 'great': 911, 'generosity': 912, 'diplomacy': 913, 'spears': 914, \"'\": 915, 'music': 916, 'videos': 917, 'content': 918, 'except': 919, 'goes': 920, 'least': 921, '90': 922, 'worse': 923, 'see': 924, 'altogether': 925, 'slight': 926, 'called': 927, 'masterpiece': 928, 'problem': 929, 'whether': 930, 'these': 931, 'ambitions': 932, 'laudable': 933, 'themselves': 934}\n",
            "935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def encode_sentence(sentence):\n",
        "    max_length = 50\n",
        "    input_ids = []\n",
        "    for item in sentence.split():\n",
        "      if item in vocab_to_int:\n",
        "        input_ids.append(vocab_to_int[item])\n",
        "      else:\n",
        "        input_ids.append(vocab_to_int['<unk>'])\n",
        "\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids += [vocab_to_int['<pad>']] * padding_length\n",
        "    return np.array(input_ids)\n",
        "\n",
        "def encode_label(label):\n",
        "    return np.array(label)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Train Dataset Encode Examples\")\n",
        "for a,b in zip(train_texts[:3],train_labels[:3]):\n",
        "  print(encode_sentence(a),b)"
      ],
      "metadata": {
        "id": "GJ0RsEBgqidq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96dab31d-9b99-49fc-f824-2163539062d3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Encode Examples\n",
            "[195  37 196  85 197   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 0\n",
            "[  2  86  12 198 199  38  29   9  87  15 200 201   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n",
            "[ 13   4 202  12 203 204   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class SimpleNN:\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "        # Initialize weights and biases\n",
        "        self.embedding_weights = np.random.rand(vocab_size, embedding_dim) # Embedding weights\n",
        "        # limit = np.sqrt(6 / (embedding_dim + hidden_size))\n",
        "        # self.W1 = np.random.uniform(-limit, limit, size=(embedding_dim, hidden_size))\n",
        "        # limit = np.sqrt(6/ (hidden_size+output_size))\n",
        "        # self.W2 = np.random.uniform(-limit, limit, size=(hidden_size, output_size))\n",
        "        limit = np.sqrt(6 / (embedding_dim + hidden_size))\n",
        "        self.W1 = np.random.uniform(-limit, limit, size=(embedding_dim, hidden_size))\n",
        "        limit = np.sqrt(6 / (hidden_size + output_size))\n",
        "        self.W2 = np.random.uniform(-limit, limit, size=(hidden_size, output_size))\n",
        "        self.b1 = np.zeros((1, hidden_size))  # Hidden layer biases\n",
        "        self.b2 = np.zeros((1, output_size))  # Output layer biases\n",
        "        self.X = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return z * (1 - z)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = np.asarray(X)\n",
        "        if X.ndim == 1 and np.issubdtype(X.dtype, np.integer):\n",
        "            emb = self.embedding_weights[X]\n",
        "            x = emb.mean(axis=0, keepdims=True)\n",
        "        else:\n",
        "            x = X.reshape(1, -1)\n",
        "        z1 = x @ self.W1 + self.b1\n",
        "        h  = np.tanh(z1)\n",
        "        z2 = h @ self.W2 + self.b2\n",
        "        output = 1.0 / (1.0 + np.exp(-z2))\n",
        "        output = np.clip(output, 1e-12, 1.0 - 1e-12)\n",
        "        self.cache = (x, h, output)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def compute_loss(self, y, output):\n",
        "        # Compute binary cross-entropy loss\n",
        "        y = np.array(y).reshape(output.shape) #\n",
        "        return -np.sum(y * np.log(output) + (1 - y) * np.log(1 - output))\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate=0.01):\n",
        "        x, h, out = self.cache\n",
        "        dz2 = out - float(y)\n",
        "        dW2 = h.T @ dz2\n",
        "        db2 = dz2\n",
        "        dh  = dz2 @ self.W2.T\n",
        "        dz1 = dh * (1.0 - h*h)\n",
        "        dW1 = x.T @ dz1\n",
        "        db1 = dz1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-76p4FBbAqLp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ykcZu_RBXe8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def train(X, y, learning_rate=0.01):\n",
        "    output = nn.forward(X)  # Forward pass\n",
        "    loss = nn.compute_loss(y, output)  # Compute loss\n",
        "    nn.backward(X, y, output, learning_rate)  # Backward pass\n",
        "    return loss\n",
        "\n",
        "def predict(x):\n",
        "    output = nn.forward(x)\n",
        "    return output, (output > 0.5).astype(int)  # Binary classification\n",
        "\n",
        "# Initialize the neural network\n",
        "vocab_size = len(vocab_to_int)  # Number of unique words in vocab\n",
        "embedding_dim = 100  # Embedding dimension\n",
        "hidden_size = 80  # Number of neurons in the hidden layer\n",
        "output_size = 1  # One output (binary classification)\n",
        "learning_rate = 0.01\n",
        "nn = SimpleNN(vocab_size, embedding_dim, hidden_size, output_size)\n",
        "\n",
        "true, pred = [], []\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    true, pred = [], []  # Reset true and predicted labels for each epoch\n",
        "    for x, y in zip(train_texts, train_labels):\n",
        "        x = encode_sentence(x)  # Encode sentence as word indices\n",
        "        train_loss += train(x, y, learning_rate)  # Train on the current sample\n",
        "        _, prediction = predict(x)  # obtain prediction\n",
        "        true.append(y)  # Append true label\n",
        "        pred.append(prediction[0][0])  # Append predicted label (extract scalar)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_acc = accuracy_score(true, pred) * 100.0\n",
        "    train_loss /= len(train_texts)  # Average training loss\n",
        "\n",
        "    # Evaluate on dev set\n",
        "    dev_true, dev_pred = [], []\n",
        "    dev_loss = 0.0\n",
        "    for x, y in zip(dev_texts, dev_labels):\n",
        "        x = encode_sentence(x)\n",
        "\n",
        "        output, prediction = predict(x)\n",
        "        dev_loss += nn.compute_loss(y,output)\n",
        "        dev_true.append(y)\n",
        "        dev_pred.append(prediction[0][0])\n",
        "    dev_loss /= len(dev_texts)\n",
        "    dev_acc = accuracy_score(dev_true, dev_pred) * 100.0\n",
        "    print(f'{epoch} epoch, train_loss = {train_loss:.4f}, train_acc: {train_acc:.2f}%, eval_loss: {dev_loss:.4f}, eval_acc: {dev_acc:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBBBnxduXd_u",
        "outputId": "061b8aab-8e60-4f6e-c230-1443e21dc289",
        "collapsed": true
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 epoch, train_loss = 0.7486, train_acc: 71.00%, eval_loss: 0.7025, eval_acc: 50.00%\n",
            "1 epoch, train_loss = 0.7433, train_acc: 76.00%, eval_loss: 0.7027, eval_acc: 50.00%\n",
            "2 epoch, train_loss = 0.7404, train_acc: 75.00%, eval_loss: 0.7029, eval_acc: 50.00%\n",
            "3 epoch, train_loss = 0.7377, train_acc: 74.00%, eval_loss: 0.7031, eval_acc: 50.00%\n",
            "4 epoch, train_loss = 0.7354, train_acc: 74.00%, eval_loss: 0.7034, eval_acc: 50.00%\n",
            "5 epoch, train_loss = 0.7334, train_acc: 73.00%, eval_loss: 0.7037, eval_acc: 50.00%\n",
            "6 epoch, train_loss = 0.7315, train_acc: 72.00%, eval_loss: 0.7040, eval_acc: 50.00%\n",
            "7 epoch, train_loss = 0.7298, train_acc: 72.00%, eval_loss: 0.7043, eval_acc: 50.00%\n",
            "8 epoch, train_loss = 0.7282, train_acc: 71.00%, eval_loss: 0.7046, eval_acc: 50.00%\n",
            "9 epoch, train_loss = 0.7268, train_acc: 71.00%, eval_loss: 0.7048, eval_acc: 50.00%\n",
            "10 epoch, train_loss = 0.7255, train_acc: 71.00%, eval_loss: 0.7051, eval_acc: 50.00%\n",
            "11 epoch, train_loss = 0.7243, train_acc: 71.00%, eval_loss: 0.7053, eval_acc: 50.00%\n",
            "12 epoch, train_loss = 0.7231, train_acc: 72.00%, eval_loss: 0.7055, eval_acc: 50.00%\n",
            "13 epoch, train_loss = 0.7221, train_acc: 73.00%, eval_loss: 0.7056, eval_acc: 50.00%\n",
            "14 epoch, train_loss = 0.7211, train_acc: 70.00%, eval_loss: 0.7058, eval_acc: 50.00%\n",
            "15 epoch, train_loss = 0.7201, train_acc: 70.00%, eval_loss: 0.7059, eval_acc: 50.00%\n",
            "16 epoch, train_loss = 0.7193, train_acc: 70.00%, eval_loss: 0.7060, eval_acc: 50.00%\n",
            "17 epoch, train_loss = 0.7184, train_acc: 69.00%, eval_loss: 0.7061, eval_acc: 50.00%\n",
            "18 epoch, train_loss = 0.7176, train_acc: 69.00%, eval_loss: 0.7062, eval_acc: 50.00%\n",
            "19 epoch, train_loss = 0.7169, train_acc: 69.00%, eval_loss: 0.7062, eval_acc: 50.00%\n",
            "20 epoch, train_loss = 0.7161, train_acc: 68.00%, eval_loss: 0.7063, eval_acc: 50.00%\n",
            "21 epoch, train_loss = 0.7154, train_acc: 66.00%, eval_loss: 0.7063, eval_acc: 50.00%\n",
            "22 epoch, train_loss = 0.7148, train_acc: 64.00%, eval_loss: 0.7063, eval_acc: 50.00%\n",
            "23 epoch, train_loss = 0.7142, train_acc: 64.00%, eval_loss: 0.7063, eval_acc: 50.00%\n",
            "24 epoch, train_loss = 0.7136, train_acc: 64.00%, eval_loss: 0.7063, eval_acc: 50.00%\n",
            "25 epoch, train_loss = 0.7130, train_acc: 64.00%, eval_loss: 0.7063, eval_acc: 50.00%\n",
            "26 epoch, train_loss = 0.7124, train_acc: 64.00%, eval_loss: 0.7063, eval_acc: 50.00%\n",
            "27 epoch, train_loss = 0.7119, train_acc: 64.00%, eval_loss: 0.7062, eval_acc: 50.00%\n",
            "28 epoch, train_loss = 0.7114, train_acc: 63.00%, eval_loss: 0.7062, eval_acc: 50.00%\n",
            "29 epoch, train_loss = 0.7109, train_acc: 63.00%, eval_loss: 0.7062, eval_acc: 50.00%\n",
            "30 epoch, train_loss = 0.7104, train_acc: 63.00%, eval_loss: 0.7061, eval_acc: 50.00%\n",
            "31 epoch, train_loss = 0.7099, train_acc: 62.00%, eval_loss: 0.7060, eval_acc: 50.00%\n",
            "32 epoch, train_loss = 0.7095, train_acc: 63.00%, eval_loss: 0.7060, eval_acc: 50.00%\n",
            "33 epoch, train_loss = 0.7091, train_acc: 62.00%, eval_loss: 0.7059, eval_acc: 50.00%\n",
            "34 epoch, train_loss = 0.7086, train_acc: 62.00%, eval_loss: 0.7058, eval_acc: 50.00%\n",
            "35 epoch, train_loss = 0.7082, train_acc: 62.00%, eval_loss: 0.7058, eval_acc: 50.00%\n",
            "36 epoch, train_loss = 0.7079, train_acc: 62.00%, eval_loss: 0.7057, eval_acc: 50.00%\n",
            "37 epoch, train_loss = 0.7075, train_acc: 62.00%, eval_loss: 0.7056, eval_acc: 50.00%\n",
            "38 epoch, train_loss = 0.7071, train_acc: 62.00%, eval_loss: 0.7055, eval_acc: 50.00%\n",
            "39 epoch, train_loss = 0.7068, train_acc: 62.00%, eval_loss: 0.7055, eval_acc: 50.00%\n",
            "40 epoch, train_loss = 0.7064, train_acc: 62.00%, eval_loss: 0.7054, eval_acc: 50.00%\n",
            "41 epoch, train_loss = 0.7061, train_acc: 62.00%, eval_loss: 0.7053, eval_acc: 50.00%\n",
            "42 epoch, train_loss = 0.7058, train_acc: 62.00%, eval_loss: 0.7052, eval_acc: 50.00%\n",
            "43 epoch, train_loss = 0.7054, train_acc: 62.00%, eval_loss: 0.7051, eval_acc: 49.00%\n",
            "44 epoch, train_loss = 0.7051, train_acc: 61.00%, eval_loss: 0.7050, eval_acc: 49.00%\n",
            "45 epoch, train_loss = 0.7048, train_acc: 61.00%, eval_loss: 0.7050, eval_acc: 50.00%\n",
            "46 epoch, train_loss = 0.7046, train_acc: 60.00%, eval_loss: 0.7049, eval_acc: 50.00%\n",
            "47 epoch, train_loss = 0.7043, train_acc: 60.00%, eval_loss: 0.7048, eval_acc: 50.00%\n",
            "48 epoch, train_loss = 0.7040, train_acc: 60.00%, eval_loss: 0.7047, eval_acc: 51.00%\n",
            "49 epoch, train_loss = 0.7037, train_acc: 60.00%, eval_loss: 0.7046, eval_acc: 51.00%\n",
            "50 epoch, train_loss = 0.7035, train_acc: 60.00%, eval_loss: 0.7046, eval_acc: 51.00%\n",
            "51 epoch, train_loss = 0.7032, train_acc: 60.00%, eval_loss: 0.7045, eval_acc: 51.00%\n",
            "52 epoch, train_loss = 0.7030, train_acc: 60.00%, eval_loss: 0.7044, eval_acc: 52.00%\n",
            "53 epoch, train_loss = 0.7027, train_acc: 60.00%, eval_loss: 0.7043, eval_acc: 52.00%\n",
            "54 epoch, train_loss = 0.7025, train_acc: 60.00%, eval_loss: 0.7043, eval_acc: 52.00%\n",
            "55 epoch, train_loss = 0.7023, train_acc: 60.00%, eval_loss: 0.7042, eval_acc: 51.00%\n",
            "56 epoch, train_loss = 0.7021, train_acc: 60.00%, eval_loss: 0.7041, eval_acc: 51.00%\n",
            "57 epoch, train_loss = 0.7018, train_acc: 60.00%, eval_loss: 0.7040, eval_acc: 52.00%\n",
            "58 epoch, train_loss = 0.7016, train_acc: 61.00%, eval_loss: 0.7040, eval_acc: 52.00%\n",
            "59 epoch, train_loss = 0.7014, train_acc: 60.00%, eval_loss: 0.7039, eval_acc: 51.00%\n",
            "60 epoch, train_loss = 0.7012, train_acc: 60.00%, eval_loss: 0.7038, eval_acc: 51.00%\n",
            "61 epoch, train_loss = 0.7010, train_acc: 60.00%, eval_loss: 0.7038, eval_acc: 50.00%\n",
            "62 epoch, train_loss = 0.7008, train_acc: 60.00%, eval_loss: 0.7037, eval_acc: 50.00%\n",
            "63 epoch, train_loss = 0.7006, train_acc: 60.00%, eval_loss: 0.7037, eval_acc: 50.00%\n",
            "64 epoch, train_loss = 0.7005, train_acc: 60.00%, eval_loss: 0.7036, eval_acc: 50.00%\n",
            "65 epoch, train_loss = 0.7003, train_acc: 60.00%, eval_loss: 0.7036, eval_acc: 50.00%\n",
            "66 epoch, train_loss = 0.7001, train_acc: 60.00%, eval_loss: 0.7035, eval_acc: 52.00%\n",
            "67 epoch, train_loss = 0.7000, train_acc: 60.00%, eval_loss: 0.7035, eval_acc: 51.00%\n",
            "68 epoch, train_loss = 0.6998, train_acc: 60.00%, eval_loss: 0.7034, eval_acc: 52.00%\n",
            "69 epoch, train_loss = 0.6996, train_acc: 60.00%, eval_loss: 0.7034, eval_acc: 52.00%\n",
            "70 epoch, train_loss = 0.6995, train_acc: 61.00%, eval_loss: 0.7033, eval_acc: 51.00%\n",
            "71 epoch, train_loss = 0.6993, train_acc: 61.00%, eval_loss: 0.7033, eval_acc: 49.00%\n",
            "72 epoch, train_loss = 0.6992, train_acc: 61.00%, eval_loss: 0.7032, eval_acc: 48.00%\n",
            "73 epoch, train_loss = 0.6991, train_acc: 61.00%, eval_loss: 0.7032, eval_acc: 48.00%\n",
            "74 epoch, train_loss = 0.6989, train_acc: 61.00%, eval_loss: 0.7032, eval_acc: 48.00%\n",
            "75 epoch, train_loss = 0.6988, train_acc: 61.00%, eval_loss: 0.7031, eval_acc: 47.00%\n",
            "76 epoch, train_loss = 0.6987, train_acc: 61.00%, eval_loss: 0.7031, eval_acc: 46.00%\n",
            "77 epoch, train_loss = 0.6985, train_acc: 62.00%, eval_loss: 0.7031, eval_acc: 46.00%\n",
            "78 epoch, train_loss = 0.6984, train_acc: 62.00%, eval_loss: 0.7030, eval_acc: 46.00%\n",
            "79 epoch, train_loss = 0.6983, train_acc: 62.00%, eval_loss: 0.7030, eval_acc: 46.00%\n",
            "80 epoch, train_loss = 0.6982, train_acc: 64.00%, eval_loss: 0.7030, eval_acc: 47.00%\n",
            "81 epoch, train_loss = 0.6981, train_acc: 64.00%, eval_loss: 0.7030, eval_acc: 47.00%\n",
            "82 epoch, train_loss = 0.6980, train_acc: 64.00%, eval_loss: 0.7030, eval_acc: 47.00%\n",
            "83 epoch, train_loss = 0.6978, train_acc: 64.00%, eval_loss: 0.7029, eval_acc: 47.00%\n",
            "84 epoch, train_loss = 0.6977, train_acc: 64.00%, eval_loss: 0.7029, eval_acc: 47.00%\n",
            "85 epoch, train_loss = 0.6976, train_acc: 64.00%, eval_loss: 0.7029, eval_acc: 47.00%\n",
            "86 epoch, train_loss = 0.6975, train_acc: 64.00%, eval_loss: 0.7029, eval_acc: 47.00%\n",
            "87 epoch, train_loss = 0.6974, train_acc: 64.00%, eval_loss: 0.7029, eval_acc: 47.00%\n",
            "88 epoch, train_loss = 0.6974, train_acc: 64.00%, eval_loss: 0.7029, eval_acc: 47.00%\n",
            "89 epoch, train_loss = 0.6973, train_acc: 64.00%, eval_loss: 0.7029, eval_acc: 48.00%\n",
            "90 epoch, train_loss = 0.6972, train_acc: 64.00%, eval_loss: 0.7028, eval_acc: 48.00%\n",
            "91 epoch, train_loss = 0.6971, train_acc: 64.00%, eval_loss: 0.7028, eval_acc: 49.00%\n",
            "92 epoch, train_loss = 0.6970, train_acc: 63.00%, eval_loss: 0.7028, eval_acc: 49.00%\n",
            "93 epoch, train_loss = 0.6969, train_acc: 63.00%, eval_loss: 0.7028, eval_acc: 48.00%\n",
            "94 epoch, train_loss = 0.6968, train_acc: 63.00%, eval_loss: 0.7028, eval_acc: 48.00%\n",
            "95 epoch, train_loss = 0.6968, train_acc: 63.00%, eval_loss: 0.7028, eval_acc: 48.00%\n",
            "96 epoch, train_loss = 0.6967, train_acc: 63.00%, eval_loss: 0.7028, eval_acc: 47.00%\n",
            "97 epoch, train_loss = 0.6966, train_acc: 63.00%, eval_loss: 0.7028, eval_acc: 46.00%\n",
            "98 epoch, train_loss = 0.6965, train_acc: 63.00%, eval_loss: 0.7028, eval_acc: 46.00%\n",
            "99 epoch, train_loss = 0.6965, train_acc: 62.00%, eval_loss: 0.7028, eval_acc: 46.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## NOTE this is test slot for check if the code is working, made by Chat GPT. I saved it just in case, comments are in Finnish\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def pick_nonempty_sample(texts, encode_sentence):\n",
        "    for t in texts:\n",
        "        x = np.asarray(encode_sentence(t))\n",
        "        if x.size > 0:\n",
        "            return x\n",
        "    return np.array([0], dtype=int)\n",
        "\n",
        "def check_hw1(nn, encode_sentence, train_texts, train_labels):\n",
        "    print(\"== HW1 forward/backward -itse­tarkistus ==\")\n",
        "\n",
        "    # 1) Muodot ja perusasetukset\n",
        "    ok = True\n",
        "    try:\n",
        "        assert getattr(nn, \"W2\").shape[1] == 1, \"W2.shape[1] pitää olla 1 (binääriulostulo)\"\n",
        "        assert getattr(nn, \"b2\").shape == (1,1), \"b2.shape pitää olla (1,1)\"\n",
        "    except AssertionError as e:\n",
        "        ok = False\n",
        "        print(\"✗\", e)\n",
        "    else:\n",
        "        print(\"✓ Ulostulokerroksen muodot ok (W2: *,1 ja b2: 1,1)\")\n",
        "\n",
        "    # 2) Forward antaa järkevän todennäköisyyden\n",
        "    xi = pick_nonempty_sample(train_texts, encode_sentence)\n",
        "    yi = float(train_labels[0])\n",
        "\n",
        "    out = nn.forward(xi)\n",
        "    out_val = float(np.squeeze(out))\n",
        "    if out.shape == (1,1) and 0.0 < out_val < 1.0:\n",
        "        print(\"✓ forward: output.shape == (1,1) ja 0<output<1\")\n",
        "    else:\n",
        "        ok = False\n",
        "        print(f\"✗ forward: odotin (1,1) ja arvo 0–1, sain shape={out.shape}, val={out_val}\")\n",
        "\n",
        "    # 3) Loss ei ole NaN/Inf\n",
        "    loss0 = nn.compute_loss(yi, out)\n",
        "    if np.isfinite(loss0):\n",
        "        print(f\"✓ compute_loss finite (loss0={float(loss0):.6f})\")\n",
        "    else:\n",
        "        ok = False\n",
        "        print(\"✗ compute_loss antoi NaN/Inf → klippaa output tai lisää eps\")\n",
        "\n",
        "    # 4) Backward päivittää vain W1,b1,W2,b2 (ei embeddingiä) ja loss laskee\n",
        "    W1o, b1o = nn.W1.copy(), nn.b1.copy()\n",
        "    W2o, b2o = nn.W2.copy(), nn.b2.copy()\n",
        "    Eo = nn.embedding_weights.copy()\n",
        "\n",
        "    nn.backward(xi, yi, out, learning_rate=0.01)\n",
        "\n",
        "    changed = {\n",
        "        \"W1\": np.any(nn.W1 != W1o),\n",
        "        \"b1\": np.any(nn.b1 != b1o),\n",
        "        \"W2\": np.any(nn.W2 != W2o),\n",
        "        \"b2\": np.any(nn.b2 != b2o),\n",
        "        \"Emb\": np.any(nn.embedding_weights != Eo),\n",
        "    }\n",
        "    print(\"Päivittyi:\", changed)\n",
        "\n",
        "    if changed[\"W1\"] and changed[\"b1\"] and changed[\"W2\"] and changed[\"b2\"] and (not changed[\"Emb\"]):\n",
        "        print(\"✓ Päivitykset vain W1,b1,W2,b2 (embedding ei päivity) \")\n",
        "    else:\n",
        "        ok = False\n",
        "        print(\"✗ Väärät parametrit päivittyivät (tai embedding päivittyi)\")\n",
        "\n",
        "    # 5) Loss ennen/jälkeen yhden askeleen\n",
        "    out1 = nn.forward(xi)\n",
        "    loss1 = nn.compute_loss(yi, out1)\n",
        "    if np.isfinite(loss1) and float(loss1) <= float(loss0):\n",
        "        print(f\"✓ loss before→after: {float(loss0):.6f} → {float(loss1):.6f}\")\n",
        "    else:\n",
        "        ok = False\n",
        "        print(f\"✗ loss ei pienentynyt: {float(loss0):.6f} → {float(loss1):.6f}\")\n",
        "\n",
        "    print(\"== Tulos:\", \"OK ✅\" if ok else \"Korjattavaa ❌\")\n",
        "    return ok\n",
        "\n",
        "check_hw1(nn, encode_sentence, train_texts, train_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqwiC5lPBeG9",
        "outputId": "1ed785ba-2098-4daf-cb7d-870b2ecd6f92"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== HW1 forward/backward -itse­tarkistus ==\n",
            "✓ Ulostulokerroksen muodot ok (W2: *,1 ja b2: 1,1)\n",
            "✓ forward: output.shape == (1,1) ja 0<output<1\n",
            "✓ compute_loss finite (loss0=0.689307)\n",
            "Päivittyi: {'W1': np.True_, 'b1': np.True_, 'W2': np.True_, 'b2': np.True_, 'Emb': np.False_}\n",
            "✓ Päivitykset vain W1,b1,W2,b2 (embedding ei päivity) \n",
            "✓ loss before→after: 0.689307 → 0.670111\n",
            "== Tulos: OK ✅\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}