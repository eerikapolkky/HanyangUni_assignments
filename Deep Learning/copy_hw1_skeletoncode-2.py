# -*- coding: utf-8 -*-
"""Copy: HW1_SkeletonCode

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bjudrR1a0HtZU3Hro2ljQGd3V8HUaxr9
"""

from google.colab import drive
drive.mount("/content/drive")

# Read file "hw1_data.tsv"
file = open("/content/drive/MyDrive/Colab Notebooks/hw1_data.tsv",'r')
ori_data = file.read().strip().split("\n")
data = []
for item in ori_data:
  item = item.split("\t")
  if len(item[0]) >0:
    data.append((item[0],item[1]))
print(len(data))

### Training data pre-processing
texts, labels = [], []
label2idx = {"0":0, "1":1}
for i, item in enumerate(data):
  text = item[0]

  ## Preprocessing (if you want to add, please add more)
  ################################################################################
  text = text.replace("  "," ") ## Replace double space
  text = text.replace(",", "") ## Replace comma to ""
  text = text.lower()  ## Lower cases
  ################################################################################
  label = label2idx[item[1]]

  texts.append(text)
  labels.append(label)

print("*"*50)
print("Total number of datasets")
print(len(texts))
print(len(labels))
print("*"*50)

#### Split into train/dev/test sets
from sklearn.model_selection import train_test_split
### Write a code for collecting samples for each class
################################################################################
pos,neg = [], []
train_texts, dev_texts, test_texts, train_labels, dev_labels, test_labels = [], [], [], [], [], []
for a,b in zip(texts,labels):
  if b == 1:
    pos.append((a,b))
  else:
    neg.append((a,b))

for a,b in pos[:50]:
  test_texts.append(a)
  test_labels.append(b)
for a,b in neg[:50]:
  test_texts.append(a)
  test_labels.append(b)

for a,b in pos[50:100]:
  dev_texts.append(a)
  dev_labels.append(b)
for a,b in neg[50:100]:
  dev_texts.append(a)
  dev_labels.append(b)

for a,b in pos[100:]:
  train_texts.append(a)
  train_labels.append(b)
for a,b in neg[100:]:
  train_texts.append(a)
  train_labels.append(b)
################################################################################


tmp = list(zip(train_texts,train_labels))
import random
random.shuffle(tmp)
train_texts, train_labels = zip(*tmp)

tmp = list(zip(dev_texts,dev_labels))
import random
random.shuffle(tmp)
dev_texts, dev_labels = zip(*tmp)

tmp = list(zip(test_texts,test_labels))
import random
random.shuffle(tmp)
test_texts, test_labels = zip(*tmp)

print("Train Dataset Examples")
print(train_texts[:3])
print(train_labels[:3])
print("*"*50)
print(train_labels)
print(dev_labels)

## Construct a vocabulary
from collections import Counter

all_words = []
for item in train_texts:
  all_words += item.split()
for item in dev_texts:
  all_words += item.split()

## Build a dictionary that maps words to integers
counts = Counter(all_words)
vocab = sorted(counts, key=counts.get, reverse=True)
vocab_to_int = {'<pad>':0, "<unk>":1}
vocab_to_int.update({word: ii for ii, word in enumerate(vocab,2)})
print(vocab_to_int)
print(len(vocab_to_int))

import numpy as np
def encode_sentence(sentence):
    max_length = 50
    input_ids = []
    for item in sentence.split():
      if item in vocab_to_int:
        input_ids.append(vocab_to_int[item])
      else:
        input_ids.append(vocab_to_int['<unk>'])

    padding_length = max_length - len(input_ids)
    input_ids += [vocab_to_int['<pad>']] * padding_length
    return np.array(input_ids)

def encode_label(label):
    return np.array(label)



print("Train Dataset Encode Examples")
for a,b in zip(train_texts[:3],train_labels[:3]):
  print(encode_sentence(a),b)

import numpy as np
from sklearn.metrics import accuracy_score

class SimpleNN:
    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):
        # Initialize weights and biases
        self.embedding_weights = np.random.rand(vocab_size, embedding_dim) # Embedding weights
        # limit = np.sqrt(6 / (embedding_dim + hidden_size))
        # self.W1 = np.random.uniform(-limit, limit, size=(embedding_dim, hidden_size))
        # limit = np.sqrt(6/ (hidden_size+output_size))
        # self.W2 = np.random.uniform(-limit, limit, size=(hidden_size, output_size))
        limit = np.sqrt(6 / (embedding_dim + hidden_size))
        self.W1 = np.random.uniform(-limit, limit, size=(embedding_dim, hidden_size))
        limit = np.sqrt(6 / (hidden_size + output_size))
        self.W2 = np.random.uniform(-limit, limit, size=(hidden_size, output_size))
        self.b1 = np.zeros((1, hidden_size))  # Hidden layer biases
        self.b2 = np.zeros((1, output_size))  # Output layer biases
        self.X = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_derivative(self, z):
        return z * (1 - z)

    def forward(self, X):
        X = np.asarray(X)
        if X.ndim == 1 and np.issubdtype(X.dtype, np.integer):
            emb = self.embedding_weights[X]
            x = emb.mean(axis=0, keepdims=True)
        else:
            x = X.reshape(1, -1)
        z1 = x @ self.W1 + self.b1
        h  = np.tanh(z1)
        z2 = h @ self.W2 + self.b2
        output = 1.0 / (1.0 + np.exp(-z2))
        output = np.clip(output, 1e-12, 1.0 - 1e-12)
        self.cache = (x, h, output)
        return output


    def compute_loss(self, y, output):
        # Compute binary cross-entropy loss
        y = np.array(y).reshape(output.shape) #
        return -np.sum(y * np.log(output) + (1 - y) * np.log(1 - output))

    def backward(self, X, y, output, learning_rate=0.01):
        x, h, out = self.cache
        dz2 = out - float(y)
        dW2 = h.T @ dz2
        db2 = dz2
        dh  = dz2 @ self.W2.T
        dz1 = dh * (1.0 - h*h)
        dW1 = x.T @ dz1
        db1 = dz1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1



import numpy as np
from sklearn.metrics import accuracy_score, f1_score

def train(X, y, learning_rate=0.01):
    output = nn.forward(X)  # Forward pass
    loss = nn.compute_loss(y, output)  # Compute loss
    nn.backward(X, y, output, learning_rate)  # Backward pass
    return loss

def predict(x):
    output = nn.forward(x)
    return output, (output > 0.5).astype(int)  # Binary classification

# Initialize the neural network
vocab_size = len(vocab_to_int)  # Number of unique words in vocab
embedding_dim = 100  # Embedding dimension
hidden_size = 80  # Number of neurons in the hidden layer
output_size = 1  # One output (binary classification)
learning_rate = 0.01
nn = SimpleNN(vocab_size, embedding_dim, hidden_size, output_size)

true, pred = [], []
epochs = 100
for epoch in range(epochs):
    train_loss = 0.0
    true, pred = [], []  # Reset true and predicted labels for each epoch
    for x, y in zip(train_texts, train_labels):
        x = encode_sentence(x)  # Encode sentence as word indices
        train_loss += train(x, y, learning_rate)  # Train on the current sample
        _, prediction = predict(x)  # obtain prediction
        true.append(y)  # Append true label
        pred.append(prediction[0][0])  # Append predicted label (extract scalar)

    # Calculate training accuracy
    train_acc = accuracy_score(true, pred) * 100.0
    train_loss /= len(train_texts)  # Average training loss

    # Evaluate on dev set
    dev_true, dev_pred = [], []
    dev_loss = 0.0
    for x, y in zip(dev_texts, dev_labels):
        x = encode_sentence(x)

        output, prediction = predict(x)
        dev_loss += nn.compute_loss(y,output)
        dev_true.append(y)
        dev_pred.append(prediction[0][0])
    dev_loss /= len(dev_texts)
    dev_acc = accuracy_score(dev_true, dev_pred) * 100.0
    print(f'{epoch} epoch, train_loss = {train_loss:.4f}, train_acc: {train_acc:.2f}%, eval_loss: {dev_loss:.4f}, eval_acc: {dev_acc:.2f}%')

## NOTE this is test slot for check if the code is working, made by Chat GPT. I saved it just in case, comments are in Finnish

import numpy as np

def pick_nonempty_sample(texts, encode_sentence):
    for t in texts:
        x = np.asarray(encode_sentence(t))
        if x.size > 0:
            return x
    # jos kaikki palauttaa tyhjää, käytä nollavektoria (vain testiä varten)
    return np.array([0], dtype=int)

def check_hw1(nn, encode_sentence, train_texts, train_labels):
    print("== HW1 forward/backward -itse­tarkistus ==")

    # 1) Muodot ja perusasetukset
    ok = True
    try:
        assert getattr(nn, "W2").shape[1] == 1, "W2.shape[1] pitää olla 1 (binääriulostulo)"
        assert getattr(nn, "b2").shape == (1,1), "b2.shape pitää olla (1,1)"
    except AssertionError as e:
        ok = False
        print("✗", e)
    else:
        print("✓ Ulostulokerroksen muodot ok (W2: *,1 ja b2: 1,1)")

    # 2) Forward antaa järkevän todennäköisyyden
    xi = pick_nonempty_sample(train_texts, encode_sentence)
    yi = float(train_labels[0])

    out = nn.forward(xi)
    out_val = float(np.squeeze(out))
    if out.shape == (1,1) and 0.0 < out_val < 1.0:
        print("✓ forward: output.shape == (1,1) ja 0<output<1")
    else:
        ok = False
        print(f"✗ forward: odotin (1,1) ja arvo 0–1, sain shape={out.shape}, val={out_val}")

    # 3) Loss ei ole NaN/Inf
    loss0 = nn.compute_loss(yi, out)
    if np.isfinite(loss0):
        print(f"✓ compute_loss finite (loss0={float(loss0):.6f})")
    else:
        ok = False
        print("✗ compute_loss antoi NaN/Inf → klippaa output tai lisää eps")

    # 4) Backward päivittää vain W1,b1,W2,b2 (ei embeddingiä) ja loss laskee
    W1o, b1o = nn.W1.copy(), nn.b1.copy()
    W2o, b2o = nn.W2.copy(), nn.b2.copy()
    Eo = nn.embedding_weights.copy()

    nn.backward(xi, yi, out, learning_rate=0.01)

    changed = {
        "W1": np.any(nn.W1 != W1o),
        "b1": np.any(nn.b1 != b1o),
        "W2": np.any(nn.W2 != W2o),
        "b2": np.any(nn.b2 != b2o),
        "Emb": np.any(nn.embedding_weights != Eo),
    }
    print("Päivittyi:", changed)

    if changed["W1"] and changed["b1"] and changed["W2"] and changed["b2"] and (not changed["Emb"]):
        print("✓ Päivitykset vain W1,b1,W2,b2 (embedding ei päivity) ")
    else:
        ok = False
        print("✗ Väärät parametrit päivittyivät (tai embedding päivittyi)")

    # 5) Loss ennen/jälkeen yhden askeleen
    out1 = nn.forward(xi)
    loss1 = nn.compute_loss(yi, out1)
    if np.isfinite(loss1) and float(loss1) <= float(loss0):
        print(f"✓ loss before→after: {float(loss0):.6f} → {float(loss1):.6f}")
    else:
        ok = False
        print(f"✗ loss ei pienentynyt: {float(loss0):.6f} → {float(loss1):.6f}")

    print("== Tulos:", "OK ✅" if ok else "Korjattavaa ❌")
    return ok

# AJA TESTI (oleta että muuttujat nn, encode_sentence, train_texts, train_labels on olemassa)
check_hw1(nn, encode_sentence, train_texts, train_labels)